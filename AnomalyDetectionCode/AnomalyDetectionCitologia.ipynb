{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad746d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-26 10:39:48.701272: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-26 10:39:48.701344: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-26 10:39:48.702517: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-26 10:39:48.791386: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "class ConvAutoencoder:\n",
    "\t@staticmethod\n",
    "\tdef build(width, height, depth, filters=(32, 64), latentDim=16):\n",
    "\t\t# initialize the input shape to be \"channels last\" along with\n",
    "\t\t# the channels dimension itself\n",
    "\t\t# channels dimension itself\n",
    "\t\tinputShape = (height, width, depth)\n",
    "\t\tchanDim = -1\n",
    "\t\t# define the input to the encoder\n",
    "\t\tinputs = Input(shape=inputShape)\n",
    "\t\tx = inputs\n",
    "\t\t# loop over the number of filters\n",
    "\t\tfor f in filters:\n",
    "\t\t\t# apply a CONV => RELU => BN operation\n",
    "\t\t\tx = Conv2D(f, (3, 3), strides=2, padding=\"same\")(x)\n",
    "\t\t\tx = LeakyReLU(alpha=0.2)(x)\n",
    "\t\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\t# flatten the network and then construct our latent vector\n",
    "\t\tvolumeSize = K.int_shape(x)\n",
    "\t\tx = Flatten()(x)\n",
    "\t\tlatent = Dense(latentDim)(x)\n",
    "\t\t# build the encoder model\n",
    "\t\tencoder = Model(inputs, latent, name=\"encoder\")\n",
    "\t\t# start building the decoder model which will accept the\n",
    "\t\t# output of the encoder as its inputs\n",
    "\t\tlatentInputs = Input(shape=(latentDim,))\n",
    "\t\tx = Dense(np.prod(volumeSize[1:]))(latentInputs)\n",
    "\t\tx = Reshape((volumeSize[1], volumeSize[2], volumeSize[3]))(x)\n",
    "\t\t# loop over our number of filters again, but this time in\n",
    "\t\t# reverse order\n",
    "\t\tfor f in filters[::-1]:\n",
    "\t\t\t# apply a CONV_TRANSPOSE => RELU => BN operation\n",
    "\t\t\tx = Conv2DTranspose(f, (3, 3), strides=2,\n",
    "\t\t\t\tpadding=\"same\")(x)\n",
    "\t\t\tx = LeakyReLU(alpha=0.2)(x)\n",
    "\t\t\tx = BatchNormalization(axis=chanDim)(x)\n",
    "\t\t# apply a single CONV_TRANSPOSE layer used to recover the\n",
    "\t\t# original depth of the image\n",
    "\t\tx = Conv2DTranspose(depth, (3, 3), padding=\"same\")(x)\n",
    "\t\toutputs = Activation(\"sigmoid\")(x)\n",
    "\t\t# build the decoder model\n",
    "\t\tdecoder = Model(latentInputs, outputs, name=\"decoder\")\n",
    "\t\t# our autoencoder is the encoder + decoder\n",
    "\t\tautoencoder = Model(inputs, decoder(encoder(inputs)),\n",
    "\t\t\tname=\"autoencoder\")\n",
    "\t\t# return a 3-tuple of the encoder, decoder, and autoencoder\n",
    "\t\treturn (encoder, decoder, autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb5bfc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "# import the necessary packages\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import mnist,cifar10\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import pickle\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "421756c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45f578d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "dirs = os.listdir('../IHQ--Output_Segmentation/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24e3eea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "traindirs = random.sample(dirs,int(len(dirs)*0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ad140fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdirs = list(set(dirs) - set(traindirs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "23af166e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainImages = []\n",
    "for folder in traindirs:\n",
    "    for im in list(paths.list_images('../IHQ--Output_Segmentation/'+folder)):\n",
    "        im1 = cv2.imread(im)\n",
    "        if((im1.shape)==(64,64,3)):\n",
    "            trainImages.append(im1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "838fe6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = np.array(trainImages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8d8c4e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('trainDatasetImages', \"wb\")\n",
    "f.write(pickle.dumps(trainDataset))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c63c4200",
   "metadata": {},
   "outputs": [],
   "source": [
    "testImages = []\n",
    "testNames = []\n",
    "for folder in testdirs:\n",
    "    for im in list(paths.list_images('../IHQ--Output_Segmentation/'+folder)):\n",
    "        im1 = cv2.imread(im)\n",
    "        if((im1.shape)==(64,64,3)):\n",
    "            testNames.append(im)\n",
    "            testImages.append(im1)\n",
    "for im in list(paths.list_images('../Malignos--Output_Segmentation/')):\n",
    "        im1 = cv2.imread(im)\n",
    "        if((im1.shape)==(64,64,3)):\n",
    "            testNames.append(im)\n",
    "            testImages.append(im1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "796b2f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataset = np.array(testImages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8c0e2c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('testDatasetImages', \"wb\")\n",
    "f.write(pickle.dumps(testDataset))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dda3edaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('testDatasetNames', \"wb\")\n",
    "f.write(pickle.dumps(testNames))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c812a163",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "INIT_LR = 1e-3\n",
    "BS = 32\n",
    "images = trainDataset\n",
    "images = images.astype(\"float32\") / 255.0\n",
    "(trainX, validX) = train_test_split(images, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c08ba1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(decoded, gt, samples=10):\n",
    "\t# initialize our list of output images\n",
    "\toutputs = None\n",
    "\t# loop over our number of output samples\n",
    "\tfor i in range(0, samples):\n",
    "\t\t# grab the original image and reconstructed image\n",
    "\t\toriginal = (gt[i] * 255).astype(\"uint8\")\n",
    "\t\trecon = (decoded[i] * 255).astype(\"uint8\")\n",
    "\t\t# stack the original and reconstructed image side-by-side\n",
    "\t\toutput = np.hstack([original, recon])\n",
    "\t\t# if the outputs array is empty, initialize it as the current\n",
    "\t\t# side-by-side image display\n",
    "\t\tif outputs is None:\n",
    "\t\t\toutputs = output\n",
    "\t\t# otherwise, vertically stack the outputs\n",
    "\t\telse:\n",
    "\t\t\toutputs = np.vstack([outputs, output])\n",
    "\t# return the output images\n",
    "\treturn outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ba806c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] building autoencoder...\n",
      "Train on 26951 samples, validate on 6738 samples\n",
      "Epoch 1/20\n",
      "26951/26951 [==============================] - 23s 866us/sample - loss: 0.0072 - val_loss: 0.0028\n",
      "Epoch 2/20\n",
      "26951/26951 [==============================] - 18s 684us/sample - loss: 0.0028 - val_loss: 0.0032\n",
      "Epoch 3/20\n",
      "26951/26951 [==============================] - 18s 682us/sample - loss: 0.0026 - val_loss: 0.0021\n",
      "Epoch 4/20\n",
      "26951/26951 [==============================] - 18s 679us/sample - loss: 0.0024 - val_loss: 0.0028\n",
      "Epoch 5/20\n",
      "26951/26951 [==============================] - 18s 681us/sample - loss: 0.0023 - val_loss: 0.0023\n",
      "Epoch 6/20\n",
      "26951/26951 [==============================] - 18s 677us/sample - loss: 0.0022 - val_loss: 0.0023\n",
      "Epoch 7/20\n",
      "26951/26951 [==============================] - 18s 681us/sample - loss: 0.0021 - val_loss: 0.0022\n",
      "Epoch 8/20\n",
      "26951/26951 [==============================] - 18s 675us/sample - loss: 0.0021 - val_loss: 0.0020\n",
      "Epoch 9/20\n",
      "26951/26951 [==============================] - 18s 676us/sample - loss: 0.0021 - val_loss: 0.0024\n",
      "Epoch 10/20\n",
      "26951/26951 [==============================] - 18s 679us/sample - loss: 0.0021 - val_loss: 0.0029\n",
      "Epoch 11/20\n",
      "26951/26951 [==============================] - 18s 680us/sample - loss: 0.0021 - val_loss: 0.0021\n",
      "Epoch 12/20\n",
      "26951/26951 [==============================] - 18s 674us/sample - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 13/20\n",
      "26951/26951 [==============================] - 18s 679us/sample - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 14/20\n",
      "26951/26951 [==============================] - 18s 677us/sample - loss: 0.0020 - val_loss: 0.0021\n",
      "Epoch 15/20\n",
      "26951/26951 [==============================] - 18s 680us/sample - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 16/20\n",
      "26951/26951 [==============================] - 18s 674us/sample - loss: 0.0020 - val_loss: 0.0020\n",
      "Epoch 17/20\n",
      "26951/26951 [==============================] - 18s 672us/sample - loss: 0.0020 - val_loss: 0.0019\n",
      "Epoch 18/20\n",
      "26951/26951 [==============================] - 18s 673us/sample - loss: 0.0020 - val_loss: 0.0022\n",
      "Epoch 19/20\n",
      "26951/26951 [==============================] - 18s 674us/sample - loss: 0.0019 - val_loss: 0.0021\n",
      "Epoch 20/20\n",
      "26951/26951 [==============================] - 18s 671us/sample - loss: 0.0019 - val_loss: 0.0020\n",
      "[INFO] making predictions...\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] building autoencoder...\")\n",
    "(encoder, decoder, autoencoder) = ConvAutoencoder.build(64, 64, 3)\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
    "autoencoder.compile(loss=\"mse\", optimizer=opt)\n",
    "# train the convolutional autoencoder\n",
    "H = autoencoder.fit(\n",
    "\ttrainX, trainX,\n",
    "\tvalidation_data=(validX, validX),\n",
    "\tepochs=EPOCHS,\n",
    "\tbatch_size=BS)\n",
    "# use the convolutional autoencoder to make predictions on the\n",
    "# testing images, construct the visualization, and then save it\n",
    "# to disk\n",
    "print(\"[INFO] making predictions...\")\n",
    "decoded = autoencoder.predict(validX)\n",
    "vis = visualize_predictions(decoded, validX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ed1ea521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(\"visualization.jpg\", vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "131fdd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] saving autoencoder...\n"
     ]
    }
   ],
   "source": [
    "N = np.arange(0, EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig('plotCitologia.jpg')\n",
    "# serialize the autoencoder model to disk\n",
    "print(\"[INFO] saving autoencoder...\")\n",
    "autoencoder.save('modelCitologia', save_format=\"h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4888d856",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
